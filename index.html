<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>TAG</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://valkyrja3607.github.io/TAG/images/overview.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1024">
    <meta property="og:image:height" content="718">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://valkyrja3607.github.io/TAG/"/>
    <meta property="og:title" content="TAG: Guidance-free Open-Vocabulary Semantic Segmentation" />
    <meta property="og:description" content="Semantic segmentation is a crucial task in computer vision, where each pixel in an image is classified into a category. However, traditional methods face significant challenges, including the need for pixel-level annotations and extensive training. Furthermore, because supervised learning uses a limited set of predefined categories, models typically struggle with rare classes and cannot recognize new ones. Unsupervised and open-vocabulary segmentation, proposed to tackle these issues, faces challenges, including the inability to assign specific class labels to clusters and the necessity of user-provided text queries for guidance. In this context, we propose a novel approach, TAG which achieves Training, Annotation, and Guidance-free open-vocabulary semantic segmentation. TAG utilizes pre-trained models such as CLIP and DINO to segment images into meaningful categories without additional training or dense annotations. It retrieves class labels from an external database, providing flexibility to adapt to new scenarios. Our TAG achieves state-of-the-art results on PascalVOC, PascalContext and ADE20K for open-vocabulary segmentation without given class names, i.e. improvement of +15.3 mIoU on PascalVOC." />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="TAG: Guidance-free Open-Vocabulary Semantic Segmentation" />
    <meta name="twitter:description" content="Semantic segmentation is a crucial task in computer vision, where each pixel in an image is classified into a category. However, traditional methods face significant challenges, including the need for pixel-level annotations and extensive training. Furthermore, because supervised learning uses a limited set of predefined categories, models typically struggle with rare classes and cannot recognize new ones. Unsupervised and open-vocabulary segmentation, proposed to tackle these issues, faces challenges, including the inability to assign specific class labels to clusters and the necessity of user-provided text queries for guidance. In this context, we propose a novel approach, TAG which achieves Training, Annotation, and Guidance-free open-vocabulary semantic segmentation. TAG utilizes pre-trained models such as CLIP and DINO to segment images into meaningful categories without additional training or dense annotations. It retrieves class labels from an external database, providing flexibility to adapt to new scenarios. Our TAG achieves state-of-the-art results on PascalVOC, PascalContext and ADE20K for open-vocabulary segmentation without given class names, i.e. improvement of +15.3 mIoU on PascalVOC." />
    <meta name="twitter:image" content="https://valkyrja3607.github.io/TAG/images/overview.png" />

    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-115292904-1', 'auto');
        ga('send', 'pageview');
    </script>
    <!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ’«</text></svg>"> -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <!-- <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@48,400,0,0" /> -->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                TAG: Guidance-free Open-Vocabulary <br> Semantic Segmentation </br>
                <!-- <small>ECCV 2024</small> -->
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li><a href="https://Valkyrja3607.github.io/">Yasufumi Kawano</a><br>Keio University</br></li>
                    <li><a href="https://www.st.keio.ac.jp/en/tprofile/elec/aoki.html">Yoshimitsu Aoki</a><br>Keio University</br></li>
                </ul>
            </div>
        </div>

        <div class="row">
                <!-- <div class="col-md-4 col-md-offset-4 text-center"> -->
                <div class="col-md-6 col-md-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <!-- <a href="https://arxiv.org/abs/"> -->
                            <image src="icon/description.svg" height="50px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/Valkyrja3607/TAG">
                            <image src="icon/github.png" height="50px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>

        <br>

        <br>
        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <div class="row">
                    <p class="text-center">
                        <img src="./images/example.png" class="img-rounded" width="80%">
                    </p>
                </div>
            </div>
        </div>
        <br>
        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Semantic segmentation is a crucial task in computer vision, where each pixel in an image is classified into a category. However, traditional methods face significant challenges, including the need for pixel-level annotations and extensive training. Furthermore, because supervised learning uses a limited set of predefined categories, models typically struggle with rare classes and cannot recognize new ones. Unsupervised and open-vocabulary segmentation, proposed to tackle these issues, faces challenges, including the inability to assign specific class labels to clusters and the necessity of user-provided text queries for guidance. In this context, we propose a novel approach, TAG which achieves Training, Annotation, and Guidance-free open-vocabulary semantic segmentation. TAG utilizes pre-trained models such as CLIP and DINO to segment images into meaningful categories without additional training or dense annotations. It retrieves class labels from an external database, providing flexibility to adapt to new scenarios. Our TAG achieves state-of-the-art results on PascalVOC, PascalContext and ADE20K for open-vocabulary segmentation without given class names, i.e. improvement of +15.3 mIoU on PascalVOC.
                </p>
            </div>
        </div>
        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    Method
                </h3>
                <p class="text-justify">
                    TAG can partition images into semantic segments and label each segment with open-vocabulary categories. First, TAG identifies segment candidates using per-pixel features obtained from DINOv2. Then, it acquires representative segment embeddings for segment candidates using per-pixel features from a ViT pre-trained with CLIP. Finally, the categories are assigned to each candidate segment by retrieving the closest matching sentence from an external database. Note that the input is only the image, with no need to input category candidates as guidance.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <div class="row">
                    <p class="text-center">
                        <img src="./images/overview.png" class="img-rounded" width="80%">
                    </p>
                </div>
            </div>
        </div>
        <br>


        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    Results
                </h3>
                <p class="text-justify">
                    We show open vocabulary semantic segmentation results.
                </p>
                <div class="row">
                    <p class="text-center">
                        <img src="./images/result.png" class="img-rounded" width="100%">
                    </p>
                </div>
                <p class="text-justify">
                    Please refer to <a href="https://arxiv.org/abs/">our paper</a> for more details.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-12"><pre>
@inproceedings{kawano2024maskdiffusion,
    title={{TAG: Guidance-free Open-Vocabulary Semantic Segmentation}},
    author={Yasufumi Kawano and Yoshimitsu Aoki},
    <!-- booktitle={}, -->
    year={2024},
    <!-- pages={00000-00000}, -->
  }</pre>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-10 col-md-offset-1">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                The website template was borrowed from <a href="https://jonbarron.info/mipnerf360/">Mip-NeRF 360</a>.
                </p>
            </div>
        </div>

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Concurrent work
                </h3>
                <ul>
                    <li><a href="https://arxiv.org/abs">MaskCLIP</a></li>
                </ul>
            </div>
        </div> -->

    </div>
    <footer class="text-center">
        &#169; Yasufumi Kawano<br />
        Last Update: 2024-03-18
    </footer>
</body>

</html>
